{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qSimm-48fPn"
   },
   "source": [
    "**DL2001 - Introduction to Data Science Lab - Midterm Exam - Fall 2024** <br>\n",
    "Please note:\n",
    "- Total marks are 40\n",
    "- Total Question are 9\n",
    "- No internet or helping material is allowed\n",
    "- You are NOT allowed to take any smart gadget (Mobile, smart watch etc.). If found, your exam will be canceled\n",
    "- Please submit your code on Cactus.\n",
    "- Add #Roll_Number as filename and download the .ipynb file, submit the file.\n",
    "- Cases of plagiarism, use of AI, unfair means like placing lab solution in the system and copying code from it or use of mobile phone will result in straight zero without any chance to defend along with dc.\n",
    "- The allowed time is **2 hours**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YeZzsWU2E1u"
   },
   "source": [
    "**Question 1:**\n",
    "Imagine you’re organizing a music playlist for a party and have a library of songs with their durations (in minutes). Create a **Dictionary** where song titles are the keys, and their durations (as floats) are the values. Write a Python program that prompts the user to enter multiple song titles (comma-separated). For each song title, check if it exists in the dictionary. If it exists, print the duration; if it doesn't, print \"Song not found.\" At the end, calculate and display the total duration of all the songs found. If no valid songs are entered, print \"No valid songs found.\"\n",
    "\n",
    "**(Marks:5)**\n",
    "\n",
    "**Song Title\t               Duration (min)**\n",
    "\n",
    "\"Blinding Lights\" =\t             3.5\n",
    "\"Levitating\"\t     =            3.2\n",
    "\"Peaches\"\t          =           3.1\n",
    "\"Save Your Tears\"\t   =          3.6\n",
    "\"Rain Drops\"\t      =         4.0\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "\n",
    "```\n",
    "Enter song titles (comma-separated): Peaches, Blinding Lights, Stay\n",
    "The duration of \"Peaches\" is 3.1 minutes\n",
    "The duration of \"Blinding Lights\" is 3.5 minutes\n",
    "Song \"Stay\" not found.\n",
    "Total duration of the valid songs: 6.6 minutes\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF1VkrEE7g3M",
    "outputId": "3e85c912-1d6c-4bae-807f-42ffbc6fac03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter song titles (comma-separated): Blinding Lights, Stay\n",
      "The duration of \"Blinding Lights\" is 3.5 minutes\n",
      "Song \"Stay\" not found.\n",
      "Total duration of the valid songs: 3.5 minutes\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "songs = {\n",
    "    \"Blinding Lights\": 3.5,\n",
    "    \"Save Your Tears\": 3.6,\n",
    "    \"Rain Drops\": 4.0,\n",
    "    \"Levitating\": 3.2,\n",
    "    \"Peaches\": 3.1,\n",
    "}\n",
    "s = input(\"Enter song titles (comma-separated): \")\n",
    "title = [title.strip() for title in s.split(\",\")]\n",
    "bool_found = False\n",
    "final = 0.0\n",
    "for title in title:\n",
    "    if title in songs:\n",
    "        print(f'The duration of \"{title}\" is {songs[title]} minutes')\n",
    "        final = final + songs[title]\n",
    "        bool_found = True\n",
    "    else:\n",
    "        print(f'Song \"{title}\" not found.')\n",
    "\n",
    "if bool_found:\n",
    "    print(f\"Total duration of the valid songs: {final} minutes\")\n",
    "else:\n",
    "    print(\"No valid songs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSsUu8gfNm_w"
   },
   "source": [
    "**Question 2: Numpy array**\n",
    "You’re working with a student grading system where each student has a unique identifier. Create a 1D array of 15 random scores between 1 and 100 to represent the test scores of 15 students. Then, organize these scores into a 5x3 matrix, where each row represents a different class with 3 students. Display both the original 1D array of scores and the reshaped matrix.\n",
    "\n",
    " **`(Marks: 3)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6cGMWNJ5Aih",
    "outputId": "fa92b70f-0f67-4e91-a968-51d1beddda68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 1D array:\n",
      "[48  1 60  6 44 47  5 68 90 46 90 64 37 98 84]\n",
      "Reshaped 5x3 matrix:\n",
      "[[48  1 60]\n",
      " [ 6 44 47]\n",
      " [ 5 68 90]\n",
      " [46 90 64]\n",
      " [37 98 84]]\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "s = np.random.randint(1, 101, size=15)\n",
    "print(\"Original 1D array:\")\n",
    "print(s)\n",
    "reshape_s = s.reshape(5, 3)\n",
    "print(\"Reshaped 5x3 matrix:\")\n",
    "print(reshape_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR4K37F13PHZ"
   },
   "source": [
    "**Question 3:**\n",
    "You’re developing a DNA sequence comparison tool to find matching nucleotide pairs in two DNA sequences. Write a function **count_matching_pairs()** that accepts two DNA sequences as arguments and returns the count of positions where both sequences contain the same two-character nucleotide pairs. For example, if the inputs are **\"ATGCCGTA\"** and **\"ATGTCGTT\"**, the function should return 3, as the two-character pairs **\"AT\"**, **\"GC\"**, and **\"GT\"** appear in the same positions in both sequences.\n",
    "\n",
    "**(Marks:5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KohQEMnJ5BS8",
    "outputId": "f6a95d8a-9757-4237-ef7d-1b687d830188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Q3\n",
    "def count_matching_pairs(str1, str2):\n",
    "    num = 0\n",
    "    if len(str1) != len(str2):\n",
    "        print(\"The sequences must have same length\")\n",
    "    for i in range(len(str1) - 1):\n",
    "        if str1[i : i + 2] == str2[i : i + 2]:\n",
    "            num = num + 1\n",
    "    return num\n",
    "\n",
    "\n",
    "str1 = \"ATGCCGTA\"\n",
    "str2 = \"ATGTCGTT\"\n",
    "ans = count_matching_pairs(str1, str2)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVa5KboC5XcY"
   },
   "source": [
    "**Question 4:**\n",
    "Create a DataFrame with columns Book, Price, and Stock:\n",
    "\n",
    "Using boolean indexing, select books with a Price greater than $8 and create a new column called TotalValue that calculates the total stock value for each book (i.e., Price * Stock). Display the resulting DataFrame with reset indexes.\n",
    "\n",
    "**(Marks: 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0jaMKlgbTLd",
    "outputId": "d6e37e8e-52a6-4304-e98e-e86c0539ab3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Book  Price  Stock  TotalValue\n",
      "0    Maths   15.5      4        62.0\n",
      "1  English   10.7     15       160.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-477b93c8999b>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['TotalValue']=new_df['Price']*new_df['Stock']\n"
     ]
    }
   ],
   "source": [
    "# Q4\n",
    "data = {\n",
    "    \"Book\": [\"Science\", \"Maths\", \"English\", \"Urdu\"],\n",
    "    \"Price\": [6.0, 15.5, 10.7, 7.5],\n",
    "    \"Stock\": [12, 4, 15, 8],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "new_df = df[df[\"Price\"] > 8]\n",
    "new_df[\"TotalValue\"] = new_df[\"Price\"] * new_df[\"Stock\"]\n",
    "result_df = new_df.reset_index(drop=True)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuxO3qD9mdYQ"
   },
   "source": [
    "**Question 5:**\n",
    "Load the data file 'exam_data_1' and  'exam_data_2 'to a dataframe and display first 10 rows of the each dataset.\n",
    "\n",
    "**(Marks:2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfIyXjHSmaYN",
    "outputId": "4dfdb0e4-fef9-4c1c-ce61-696c343b485a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First dataset:         Name   Age Date of Birth  Height (cm)  Weight (kg)   Salary\n",
      "0     Alice  30.0      1/1/1994        165.0         60.0  $70,000\n",
      "1       Bob   NaN      1/1/1999        175.5          NaN  $60,000\n",
      "2   Charlie  35.0           NaN        180.2         80.0      NaN\n",
      "3     David  28.0      1/1/1996        195.0         75.0  $65,000\n",
      "4       Eva  22.0      1/1/2002        160.4         50.0      NaN\n",
      "5     Frank  33.0      1/1/1989        172.0         65.0  $72,000\n",
      "6      Gina  26.0           NaN        165.5          NaN  $58,000\n",
      "7     Henry  29.0      1/1/1995        168.5         70.0      NaN\n",
      "8       Ivy   NaN      1/1/2000        100.0         66.5  $63,000\n",
      "9      Jack  25.0      1/1/1999        177.5         68.0  $59,000\n",
      "10    Karen  27.0     5/15/1992        163.0         72.0  $62,000\n",
      "11      Leo  32.0     3/22/1988        171.0         60.0  $85,000\n",
      "12     Mona   NaN     7/12/1997        180.0         78.0      NaN\n",
      "13     Nina  24.0           NaN          NaN          NaN  $54,000\n",
      "14    Oscar  31.0    11/10/1991        160.5         63.0  $68,000\n",
      "15     Paul  36.0      9/8/1990        220.0         90.0  $80,000\n",
      "16   Quincy  29.0      1/5/1993        155.0         58.0      NaN\n",
      "17   Rachel   NaN    12/25/2001        170.0          NaN  $60,000\n",
      "18    Steve  34.0           NaN        185.0         85.0  $78,000\n",
      "19     Tina  23.0     4/13/2003        178.5         67.0  $55,000\n",
      "\n",
      "Second dataset:         Name  Department   Join Date   Salary       Location\n",
      "0     Alice          HR  2020-02-01  $68,000       New York\n",
      "1   Charlie     Finance  2019-05-15  $82,000        Chicago\n",
      "2     David          IT  2021-09-10  $67,500  San Francisco\n",
      "3     Frank   Marketing  2018-11-30  $75,000    Los Angeles\n",
      "4     Henry       Sales  2020-12-10      NaN         Boston\n",
      "5     Julia  Operations  2019-01-20  $55,000        Seattle\n",
      "6       Ivy          IT  2018-03-10  $60,000         Austin\n",
      "7     Kevin     Finance  2020-06-30      NaN         Denver\n",
      "8     Laura   Marketing  2019-12-12  $62,000          Miami\n",
      "9     Nancy          HR  2021-07-01  $65,000         Dallas\n",
      "10    Oscar          IT  2017-05-18  $72,500      San Diego\n",
      "11     Paul       Sales  2022-03-21  $85,000        Orlando\n",
      "12   Quincy     Finance  2020-08-14      NaN        Houston\n",
      "13   Rachel          HR  2018-10-23  $70,000        Detroit\n",
      "14    Steve   Marketing  2019-06-11  $77,000       Portland\n",
      "15     Tina          IT  2020-09-19  $64,000        Phoenix\n",
      "16      Uma       Sales  2019-02-27      NaN      Baltimore\n",
      "17   Victor          HR  2018-12-05  $58,000        Atlanta\n",
      "18    Wendy  Operations  2021-04-06  $59,000      Nashville\n",
      "19   Xander     Finance  2019-03-08  $80,000      Cleveland\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "df_1 = pd.read_csv(\"exam_data_1.csv\")\n",
    "df_2 = pd.read_csv(\"exam_data_2.csv\")\n",
    "print(\"\\nFirst dataset: \", df_1)\n",
    "print(\"\\nSecond dataset: \", df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX9ga8XyrKOo"
   },
   "source": [
    "**Question 6:**\n",
    "\n",
    "You have two datasets:\n",
    "\n",
    "1.   **exam_data_1.csv** - Contains personal and physical attributes of employees, including columns:\n",
    "**Name, Age, Date of Birth, Height (cm), Weight (kg), Salary**\n",
    "2.   **exam_data_2.csv** - Contains professional details of employees, including columns:\n",
    "**Name, Department, Join Date, Salary, Location**\n",
    "\n",
    "\n",
    "Write a Python program to:\n",
    "\n",
    "**1)**Load both datasets into separate DataFrames.\n",
    "\n",
    "**2)**Merge the two DataFrames on the Name column to create a combined DataFrame that includes all records from both datasets, even if an employee is missing from one of them (use an outer join).\n",
    "\n",
    "**3)**Finally, display the merged DataFrame.\n",
    "\n",
    "**(Marks: 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcWpOrMh8QZ1",
    "outputId": "568fb4bc-c28d-4e44-8cc1-601f3d642848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Name   Age Date of Birth  Height (cm)  Weight (kg) Salary_x  \\\n",
      "0     Alice  30.0      1/1/1994        165.0         60.0  $70,000   \n",
      "1       Bob   NaN      1/1/1999        175.5          NaN  $60,000   \n",
      "2   Charlie  35.0           NaN        180.2         80.0      NaN   \n",
      "3     David  28.0      1/1/1996        195.0         75.0  $65,000   \n",
      "4       Eva  22.0      1/1/2002        160.4         50.0      NaN   \n",
      "5     Frank  33.0      1/1/1989        172.0         65.0  $72,000   \n",
      "6      Gina  26.0           NaN        165.5          NaN  $58,000   \n",
      "7     Henry  29.0      1/1/1995        168.5         70.0      NaN   \n",
      "8       Ivy   NaN      1/1/2000        100.0         66.5  $63,000   \n",
      "9      Jack  25.0      1/1/1999        177.5         68.0  $59,000   \n",
      "10    Karen  27.0     5/15/1992        163.0         72.0  $62,000   \n",
      "11      Leo  32.0     3/22/1988        171.0         60.0  $85,000   \n",
      "12     Mona   NaN     7/12/1997        180.0         78.0      NaN   \n",
      "13     Nina  24.0           NaN          NaN          NaN  $54,000   \n",
      "14    Oscar  31.0    11/10/1991        160.5         63.0  $68,000   \n",
      "15     Paul  36.0      9/8/1990        220.0         90.0  $80,000   \n",
      "16   Quincy  29.0      1/5/1993        155.0         58.0      NaN   \n",
      "17   Rachel   NaN    12/25/2001        170.0          NaN  $60,000   \n",
      "18    Steve  34.0           NaN        185.0         85.0  $78,000   \n",
      "19     Tina  23.0     4/13/2003        178.5         67.0  $55,000   \n",
      "20    Julia   NaN           NaN          NaN          NaN      NaN   \n",
      "21    Kevin   NaN           NaN          NaN          NaN      NaN   \n",
      "22    Laura   NaN           NaN          NaN          NaN      NaN   \n",
      "23    Nancy   NaN           NaN          NaN          NaN      NaN   \n",
      "24      Uma   NaN           NaN          NaN          NaN      NaN   \n",
      "25   Victor   NaN           NaN          NaN          NaN      NaN   \n",
      "26    Wendy   NaN           NaN          NaN          NaN      NaN   \n",
      "27   Xander   NaN           NaN          NaN          NaN      NaN   \n",
      "\n",
      "    Department   Join Date Salary_y       Location  \n",
      "0           HR  2020-02-01  $68,000       New York  \n",
      "1          NaN         NaN      NaN            NaN  \n",
      "2      Finance  2019-05-15  $82,000        Chicago  \n",
      "3           IT  2021-09-10  $67,500  San Francisco  \n",
      "4          NaN         NaN      NaN            NaN  \n",
      "5    Marketing  2018-11-30  $75,000    Los Angeles  \n",
      "6          NaN         NaN      NaN            NaN  \n",
      "7        Sales  2020-12-10      NaN         Boston  \n",
      "8           IT  2018-03-10  $60,000         Austin  \n",
      "9          NaN         NaN      NaN            NaN  \n",
      "10         NaN         NaN      NaN            NaN  \n",
      "11         NaN         NaN      NaN            NaN  \n",
      "12         NaN         NaN      NaN            NaN  \n",
      "13         NaN         NaN      NaN            NaN  \n",
      "14          IT  2017-05-18  $72,500      San Diego  \n",
      "15       Sales  2022-03-21  $85,000        Orlando  \n",
      "16     Finance  2020-08-14      NaN        Houston  \n",
      "17          HR  2018-10-23  $70,000        Detroit  \n",
      "18   Marketing  2019-06-11  $77,000       Portland  \n",
      "19          IT  2020-09-19  $64,000        Phoenix  \n",
      "20  Operations  2019-01-20  $55,000        Seattle  \n",
      "21     Finance  2020-06-30      NaN         Denver  \n",
      "22   Marketing  2019-12-12  $62,000          Miami  \n",
      "23          HR  2021-07-01  $65,000         Dallas  \n",
      "24       Sales  2019-02-27      NaN      Baltimore  \n",
      "25          HR  2018-12-05  $58,000        Atlanta  \n",
      "26  Operations  2021-04-06  $59,000      Nashville  \n",
      "27     Finance  2019-03-08  $80,000      Cleveland  \n"
     ]
    }
   ],
   "source": [
    "# Q6\n",
    "df_1 = pd.read_csv(\"exam_data_1.csv\")\n",
    "df_2 = pd.read_csv(\"exam_data_2.csv\")\n",
    "\n",
    "merged_data = pd.merge(df_1, df_2, on=\"Name\", how=\"outer\")\n",
    "merged_data.to_csv(\"merged_dataset.csv\")\n",
    "print(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWvPvMxFpBbW"
   },
   "source": [
    "**Question 7:** Use both the **Interquartile Range (IQR)** method and the **Z-score** method to identify outliers in the Weight (kg) column. Show the steps used to calculate the IQR and the bounds for identifying outliers using both methods. List the names and weights of any outliers you find.**(5 Marks)**\n",
    "\n",
    "Explain why they are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00qDh33viFl7",
    "outputId": "6d638ac3-5b05-46fc-cc81-eaa15c51f4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers using IQR method:\n",
      "Empty DataFrame\n",
      "Columns: [Name, Weight (kg)]\n",
      "Index: []\n",
      "Outliers using Z-score method:\n",
      "Empty DataFrame\n",
      "Columns: [Name, Weight (kg)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Q7\n",
    "df = pd.DataFrame(merged_data)\n",
    "\n",
    "# IQR Method\n",
    "q1 = df[\"Weight (kg)\"].quantile(0.25)\n",
    "q3 = df[\"Weight (kg)\"].quantile(0.75)\n",
    "IQR = q3 - q1\n",
    "\n",
    "# IQR Method\n",
    "iqr_outliers = df[\n",
    "    (df[\"Weight (kg)\"] < q1 - 1.5 * IQR) | (df[\"Weight (kg)\"] > q3 + 1.5 * IQR)\n",
    "]\n",
    "print(\"Outliers using IQR method:\")\n",
    "print(iqr_outliers[[\"Name\", \"Weight (kg)\"]])\n",
    "\n",
    "# Z-score Method\n",
    "df[\"Z-score\"] = stats.zscore(df[\"Weight (kg)\"])\n",
    "z_score_outliers = df[(df[\"Z-score\"] < -3) | (df[\"Z-score\"] > 3)]\n",
    "print(\"Outliers using Z-score method:\")\n",
    "print(z_score_outliers[[\"Name\", \"Weight (kg)\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M4P-AJK2EiZ"
   },
   "source": [
    "**Question 8:**\n",
    "Using the merged dataset created from **dataset1_expanded.csv** and **dataset2_expanded.csv**, write a Python program to handle missing values effectively.\n",
    "\n",
    "**Perform the following steps:**\n",
    "\n",
    "\n",
    "1.  **Drop Rows with Missing Values:** Create a DataFrame from the merged dataset that excludes any rows with missing values in any column, and display the result.\n",
    "2.   **Fill Missing Values:**\n",
    "\n",
    "\n",
    "*   For numerical columns (Age, Height (cm), Weight (kg)), replace missing values with the mean of each respective column\n",
    "*   For categorical columns (Department, Location), replace missing values with the most frequent value in that column.\n",
    "*   Display the resulting DataFrame after applying these methods.\n",
    "\n",
    "\n",
    "3.   **Forward Fill Remaining Missing Values:** Use forward filling to fill in any remaining missing values, and display the final DataFrame.\n",
    "**(Marks: 6)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P3iv0Gn8TAe",
    "outputId": "904564a5-a2f8-4355-c3ac-ea7f15565396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping rows with missing values:\n",
      "    Unnamed: 0   Name   Age Date of Birth  Height (cm)  Weight (kg) Salary_x  \\\n",
      "0            0  Alice  30.0      1/1/1994        165.0         60.0  $70,000   \n",
      "3            3  David  28.0      1/1/1996        195.0         75.0  $65,000   \n",
      "5            5  Frank  33.0      1/1/1989        172.0         65.0  $72,000   \n",
      "14          14  Oscar  31.0    11/10/1991        160.5         63.0  $68,000   \n",
      "15          15   Paul  36.0      9/8/1990        220.0         90.0  $80,000   \n",
      "19          19   Tina  23.0     4/13/2003        178.5         67.0  $55,000   \n",
      "\n",
      "   Department   Join Date Salary_y       Location  \n",
      "0          HR  2020-02-01  $68,000       New York  \n",
      "3          IT  2021-09-10  $67,500  San Francisco  \n",
      "5   Marketing  2018-11-30  $75,000    Los Angeles  \n",
      "14         IT  2017-05-18  $72,500      San Diego  \n",
      "15      Sales  2022-03-21  $85,000        Orlando  \n",
      "19         IT  2020-09-19  $64,000        Phoenix  \n",
      "DataFrame after missing values fill:\n",
      "    Unnamed: 0     Name   Age Date of Birth  Height (cm)  Weight (kg)  \\\n",
      "0            0    Alice  30.0      1/1/1994   165.000000     60.00000   \n",
      "1            1      Bob  29.0      1/1/1999   175.500000     69.21875   \n",
      "2            2  Charlie  35.0           NaN   180.200000     80.00000   \n",
      "3            3    David  28.0      1/1/1996   195.000000     75.00000   \n",
      "4            4      Eva  22.0      1/1/2002   160.400000     50.00000   \n",
      "5            5    Frank  33.0      1/1/1989   172.000000     65.00000   \n",
      "6            6     Gina  26.0           NaN   165.500000     69.21875   \n",
      "7            7    Henry  29.0      1/1/1995   168.500000     70.00000   \n",
      "8            8      Ivy  29.0      1/1/2000   100.000000     66.50000   \n",
      "9            9     Jack  25.0      1/1/1999   177.500000     68.00000   \n",
      "10          10    Karen  27.0     5/15/1992   163.000000     72.00000   \n",
      "11          11      Leo  32.0     3/22/1988   171.000000     60.00000   \n",
      "12          12     Mona  29.0     7/12/1997   180.000000     78.00000   \n",
      "13          13     Nina  24.0           NaN   170.663158     69.21875   \n",
      "14          14    Oscar  31.0    11/10/1991   160.500000     63.00000   \n",
      "15          15     Paul  36.0      9/8/1990   220.000000     90.00000   \n",
      "16          16   Quincy  29.0      1/5/1993   155.000000     58.00000   \n",
      "17          17   Rachel  29.0    12/25/2001   170.000000     69.21875   \n",
      "18          18    Steve  34.0           NaN   185.000000     85.00000   \n",
      "19          19     Tina  23.0     4/13/2003   178.500000     67.00000   \n",
      "20          20    Julia  29.0           NaN   170.663158     69.21875   \n",
      "21          21    Kevin  29.0           NaN   170.663158     69.21875   \n",
      "22          22    Laura  29.0           NaN   170.663158     69.21875   \n",
      "23          23    Nancy  29.0           NaN   170.663158     69.21875   \n",
      "24          24      Uma  29.0           NaN   170.663158     69.21875   \n",
      "25          25   Victor  29.0           NaN   170.663158     69.21875   \n",
      "26          26    Wendy  29.0           NaN   170.663158     69.21875   \n",
      "27          27   Xander  29.0           NaN   170.663158     69.21875   \n",
      "\n",
      "   Salary_x  Department   Join Date Salary_y       Location  \n",
      "0   $70,000          HR  2020-02-01  $68,000       New York  \n",
      "1   $60,000     Finance         NaN      NaN        Atlanta  \n",
      "2       NaN     Finance  2019-05-15  $82,000        Chicago  \n",
      "3   $65,000          IT  2021-09-10  $67,500  San Francisco  \n",
      "4       NaN     Finance         NaN      NaN        Atlanta  \n",
      "5   $72,000   Marketing  2018-11-30  $75,000    Los Angeles  \n",
      "6   $58,000     Finance         NaN      NaN        Atlanta  \n",
      "7       NaN       Sales  2020-12-10      NaN         Boston  \n",
      "8   $63,000          IT  2018-03-10  $60,000         Austin  \n",
      "9   $59,000     Finance         NaN      NaN        Atlanta  \n",
      "10  $62,000     Finance         NaN      NaN        Atlanta  \n",
      "11  $85,000     Finance         NaN      NaN        Atlanta  \n",
      "12      NaN     Finance         NaN      NaN        Atlanta  \n",
      "13  $54,000     Finance         NaN      NaN        Atlanta  \n",
      "14  $68,000          IT  2017-05-18  $72,500      San Diego  \n",
      "15  $80,000       Sales  2022-03-21  $85,000        Orlando  \n",
      "16      NaN     Finance  2020-08-14      NaN        Houston  \n",
      "17  $60,000          HR  2018-10-23  $70,000        Detroit  \n",
      "18  $78,000   Marketing  2019-06-11  $77,000       Portland  \n",
      "19  $55,000          IT  2020-09-19  $64,000        Phoenix  \n",
      "20      NaN  Operations  2019-01-20  $55,000        Seattle  \n",
      "21      NaN     Finance  2020-06-30      NaN         Denver  \n",
      "22      NaN   Marketing  2019-12-12  $62,000          Miami  \n",
      "23      NaN          HR  2021-07-01  $65,000         Dallas  \n",
      "24      NaN       Sales  2019-02-27      NaN      Baltimore  \n",
      "25      NaN          HR  2018-12-05  $58,000        Atlanta  \n",
      "26      NaN  Operations  2021-04-06  $59,000      Nashville  \n",
      "27      NaN     Finance  2019-03-08  $80,000      Cleveland  \n",
      "Final DataFrame after forward filling remaining missed values:\n",
      "    Unnamed: 0     Name   Age Date of Birth  Height (cm)  Weight (kg)  \\\n",
      "0            0    Alice  30.0      1/1/1994   165.000000     60.00000   \n",
      "1            1      Bob  29.0      1/1/1999   175.500000     69.21875   \n",
      "2            2  Charlie  35.0      1/1/1999   180.200000     80.00000   \n",
      "3            3    David  28.0      1/1/1996   195.000000     75.00000   \n",
      "4            4      Eva  22.0      1/1/2002   160.400000     50.00000   \n",
      "5            5    Frank  33.0      1/1/1989   172.000000     65.00000   \n",
      "6            6     Gina  26.0      1/1/1989   165.500000     69.21875   \n",
      "7            7    Henry  29.0      1/1/1995   168.500000     70.00000   \n",
      "8            8      Ivy  29.0      1/1/2000   100.000000     66.50000   \n",
      "9            9     Jack  25.0      1/1/1999   177.500000     68.00000   \n",
      "10          10    Karen  27.0     5/15/1992   163.000000     72.00000   \n",
      "11          11      Leo  32.0     3/22/1988   171.000000     60.00000   \n",
      "12          12     Mona  29.0     7/12/1997   180.000000     78.00000   \n",
      "13          13     Nina  24.0     7/12/1997   170.663158     69.21875   \n",
      "14          14    Oscar  31.0    11/10/1991   160.500000     63.00000   \n",
      "15          15     Paul  36.0      9/8/1990   220.000000     90.00000   \n",
      "16          16   Quincy  29.0      1/5/1993   155.000000     58.00000   \n",
      "17          17   Rachel  29.0    12/25/2001   170.000000     69.21875   \n",
      "18          18    Steve  34.0    12/25/2001   185.000000     85.00000   \n",
      "19          19     Tina  23.0     4/13/2003   178.500000     67.00000   \n",
      "20          20    Julia  29.0     4/13/2003   170.663158     69.21875   \n",
      "21          21    Kevin  29.0     4/13/2003   170.663158     69.21875   \n",
      "22          22    Laura  29.0     4/13/2003   170.663158     69.21875   \n",
      "23          23    Nancy  29.0     4/13/2003   170.663158     69.21875   \n",
      "24          24      Uma  29.0     4/13/2003   170.663158     69.21875   \n",
      "25          25   Victor  29.0     4/13/2003   170.663158     69.21875   \n",
      "26          26    Wendy  29.0     4/13/2003   170.663158     69.21875   \n",
      "27          27   Xander  29.0     4/13/2003   170.663158     69.21875   \n",
      "\n",
      "   Salary_x  Department   Join Date Salary_y       Location  \n",
      "0   $70,000          HR  2020-02-01  $68,000       New York  \n",
      "1   $60,000     Finance  2020-02-01  $68,000        Atlanta  \n",
      "2   $60,000     Finance  2019-05-15  $82,000        Chicago  \n",
      "3   $65,000          IT  2021-09-10  $67,500  San Francisco  \n",
      "4   $65,000     Finance  2021-09-10  $67,500        Atlanta  \n",
      "5   $72,000   Marketing  2018-11-30  $75,000    Los Angeles  \n",
      "6   $58,000     Finance  2018-11-30  $75,000        Atlanta  \n",
      "7   $58,000       Sales  2020-12-10  $75,000         Boston  \n",
      "8   $63,000          IT  2018-03-10  $60,000         Austin  \n",
      "9   $59,000     Finance  2018-03-10  $60,000        Atlanta  \n",
      "10  $62,000     Finance  2018-03-10  $60,000        Atlanta  \n",
      "11  $85,000     Finance  2018-03-10  $60,000        Atlanta  \n",
      "12  $85,000     Finance  2018-03-10  $60,000        Atlanta  \n",
      "13  $54,000     Finance  2018-03-10  $60,000        Atlanta  \n",
      "14  $68,000          IT  2017-05-18  $72,500      San Diego  \n",
      "15  $80,000       Sales  2022-03-21  $85,000        Orlando  \n",
      "16  $80,000     Finance  2020-08-14  $85,000        Houston  \n",
      "17  $60,000          HR  2018-10-23  $70,000        Detroit  \n",
      "18  $78,000   Marketing  2019-06-11  $77,000       Portland  \n",
      "19  $55,000          IT  2020-09-19  $64,000        Phoenix  \n",
      "20  $55,000  Operations  2019-01-20  $55,000        Seattle  \n",
      "21  $55,000     Finance  2020-06-30  $55,000         Denver  \n",
      "22  $55,000   Marketing  2019-12-12  $62,000          Miami  \n",
      "23  $55,000          HR  2021-07-01  $65,000         Dallas  \n",
      "24  $55,000       Sales  2019-02-27  $65,000      Baltimore  \n",
      "25  $55,000          HR  2018-12-05  $58,000        Atlanta  \n",
      "26  $55,000  Operations  2021-04-06  $59,000      Nashville  \n",
      "27  $55,000     Finance  2019-03-08  $80,000      Cleveland  \n"
     ]
    }
   ],
   "source": [
    "# Q8\n",
    "merged_df = pd.read_csv(\"merged_dataset.csv\")\n",
    "df_1 = merged_df.dropna()\n",
    "print(\"DataFrame after dropping rows with missing values:\")\n",
    "print(df_1)\n",
    "\n",
    "num_cols = [\"Age\", \"Height (cm)\", \"Weight (kg)\"]\n",
    "for col in num_cols:\n",
    "    merged_df[col].fillna(merged_df[col].mean(), inplace=True)\n",
    "\n",
    "cat_cols = [\"Department\", \"Location\"]\n",
    "for col in cat_cols:\n",
    "    merged_df[col].fillna(merged_df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(\"DataFrame after missing values fill:\")\n",
    "print(merged_df)\n",
    "merged_df.ffill(inplace=True)\n",
    "\n",
    "print(\"Final DataFrame after forward filling remaining missed values:\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaWYc5Uv4NOH"
   },
   "source": [
    "**Question 9:**\n",
    "Using the merged dataset created from dataset1_expanded.csv and dataset2_expanded.csv, perform the following data type conversions:\n",
    "\n",
    "1. Convert the data type of the Date of Birth column to DateTime format.\n",
    "2. Convert the data type of the Weight (kg) column to integer.\n",
    "3. Verifies and displays the data types of the Date of Birth and Weight (kg) columns after conversion.\n",
    "**(Marks: 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFNbSWKG8YYB",
    "outputId": "1186b346-b012-4b98-baf2-466cf81c372f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of Birth    datetime64[ns]\n",
      "Weight (kg)             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Q9\n",
    "new_df = pd.read_csv(\"merged_dataset.csv\")\n",
    "new_df[\"Date of Birth\"] = pd.to_datetime(new_df[\"Date of Birth\"])\n",
    "new_df[\"Weight (kg)\"] = pd.to_numeric(new_df[\"Weight (kg)\"])\n",
    "print(new_df[[\"Date of Birth\", \"Weight (kg)\"]].dtypes)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
